{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "J-SOCfgJtHJw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1505/4068143885.py:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-notebook')\n",
            "/tmp/ipykernel_1505/4068143885.py:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-whitegrid')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "outputs": [],
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nvTUR66UtPUk"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)] # Initialize state values.\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1 # Ensure valid noise value.\n",
        "        self.create_next_values() # Initialize the next state values.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #get the row and column from the state\n",
        "        state_r, state_c = self._state_to_rc(state)\n",
        "\n",
        "        #get the offset from action to apply to the row and column of the state\n",
        "        offset_r, offset_c = action_to_offset[action]\n",
        "\n",
        "        #get the new row and column after applying the offsets \n",
        "        new_r, new_c = state_r + offset_r, state_c + offset_c\n",
        "\n",
        "        # if the new row and column are in bounds\n",
        "        if self._inbounds_rc(new_r, new_c) and state not in self._blocked_cells:\n",
        "            #get the new state from the new row and new column\n",
        "            new_state = new_r * self._width + new_c\n",
        "            return new_state\n",
        "        else:\n",
        "            return state\n",
        "        \n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        #To Do:\n",
        "        if state == self._goal_cell or state in self._danger_cells:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #create a an array to store the non-terminal states\n",
        "        non_terminal_state = []\n",
        "        #loop through all states in the grid\n",
        "        for state in range(len(self._grid_values)):\n",
        "            #if the state is not terminal then add to non_terminal_state array\n",
        "            if self.is_terminal(state) == False and state not in self._blocked_cells:\n",
        "                non_terminal_state.append(state)\n",
        "        #return non_terminal_state array\n",
        "        return non_terminal_state\n",
        "    \n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #create an array to store valid actions\n",
        "        if self.is_terminal(state):\n",
        "            return []\n",
        "        else:\n",
        "            return list(Action)\n",
        "            \n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        #TO DO:\n",
        "        #if the state is in a goal cell (non-zero reward) return the reward value\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        #if the state is in a danger cell (non-zero reward) return the reward value\n",
        "        elif state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        # else return 0 as there is no reward\n",
        "        else:\n",
        "            return 0 \n",
        "        \n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state\n",
        "        Each item in the list is a dictionary, containing the probability of reaching that state and the state itself\n",
        "        \"\"\"\n",
        "        transactions = []\n",
        "        #this is the state that will be used to calculate the probability of reaching the new state\n",
        "        new_state = self._state_from_action(state, action)\n",
        "\n",
        "        # if there is no noise then the new state is deterministic \n",
        "        if self._noise == 0:\n",
        "            transactions.append({new_state: 1.0})\n",
        "        else:\n",
        "            # we need to calculate the probability of reaching the new state\n",
        "            noise_prob = self._noise / (len(list(Action)) - 1)\n",
        "            # add the new state with the probaility of reaching it\n",
        "            transactions.append({new_state: 1-self._noise})\n",
        "\n",
        "            # loop through all the other actions and calculate the probability of reaching the new state\n",
        "            for noise_action in list(Action):                \n",
        "                # if the noise action is not the same as the action then calculate the probability of reaching the new state\n",
        "                if noise_action != action:\n",
        "                    # get the new state from the noise action\n",
        "                    noisy_state = self._state_from_action(state, noise_action)\n",
        "                    # add the new state with the probaility of reaching it\n",
        "                    transactions.append({noisy_state: noise_prob})\n",
        "        return transactions\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating\n",
        "        If this is not used, then asynchronous updating may result in unexpected results\n",
        "        To use properly, run this at the start of each iteration\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #create a copy of the grid values\n",
        "        self._next_values = deepcopy(self._grid_values)\n",
        "\n",
        "\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values\n",
        "        To use properly, run this at the end of each iteration\n",
        "        \"\"\"\n",
        "        # TO DO:\n",
        "        #set the grid values to the next values\n",
        "        self._grid_values = self._next_values\n",
        "        \n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy\n",
        "        This value will not update into main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        #TO DO:\n",
        "        #set the value of the state to the next values\n",
        "        self._next_values[state] = value\n",
        "\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        #To Do:\n",
        "        #grab every state not just non-teminal states\n",
        "        # non_terminal_states = self.get_states()\n",
        "        create_next_values = self.create_next_values()\n",
        "        states = range(self._height * self._width)\n",
        "        num_states = len(states)\n",
        "\n",
        "        A = np.zeros((num_states, num_states))\n",
        "        b = np.zeros(num_states)\n",
        "\n",
        "        for state in states:\n",
        "            reward = self.get_reward(state)\n",
        "            if self.is_terminal(state):\n",
        "                A[state, state] = 1\n",
        "                b[state] = self.get_reward(state)\n",
        "            else:\n",
        "                valid_actions = self.get_actions(state)\n",
        "\n",
        "                for action in valid_actions:\n",
        "                    transactions = self.get_transitions(state, action)\n",
        "\n",
        "                    for transaction in transactions:\n",
        "                        new_state, prob = list(transaction.items())[0]\n",
        "                        total_prob = prob * 1/(len(list(Action)))\n",
        "\n",
        "                        A[state, new_state] -= total_prob * discount_factor\n",
        "                        b[state] += discount_factor * total_prob * self.get_reward(new_state)\n",
        "            A[state,state] += 1\n",
        "\n",
        "        values = np.linalg.solve(A, b)\n",
        "        for i, state in enumerate(states):\n",
        "            self.set_value(state, values[i])\n",
        "        \n",
        "        self.set_next_values()\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Dfgo0v5sNO78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -0.63  -1.88  -5.00  -1.73   0.00 \n",
            " -0.22   ----   ----   0.00   1.73 \n",
            " -0.31   ----   ----   0.00   GOAL \n",
            " -1.15  -2.06  -2.33  -5.00  -0.19 \n",
            " -2.59  -5.00  -2.86  -2.58  -0.83 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "# Solve the linear system\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.75)\n",
        "print(values_grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HJI-fizhOIM-"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (2476411251.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[33], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    #TO DO\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(gw, discount, tolerance=0.1):\n",
        "    #TO DO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwueBZR8tPXE"
      },
      "outputs": [],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yli-IAo6tPZU"
      },
      "outputs": [],
      "source": [
        "value_iteration(simple_gw, discount, 0.1)\n",
        "print(simple_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUo6u5kutPbm"
      },
      "outputs": [],
      "source": [
        "value_iteration(noisy_gw, discount, 0.1)\n",
        "print(noisy_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uXK_mV9tPdp"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(gw, discount, tolerance=0.1):\n",
        "    #TO DO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
