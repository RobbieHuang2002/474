{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J-SOCfgJtHJw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.style.use('seaborn-notebook')\n",
        "# plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "outputs": [],
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nvTUR66UtPUk"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)] # Initialize state values.\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1 # Ensure valid noise value.\n",
        "        self.create_next_values() # Initialize the next state values.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #get the row and column from the state\n",
        "        state_r, state_c = self._state_to_rc(state)\n",
        "\n",
        "        #get the offset from action to apply to the row and column of the state\n",
        "        offset_r, offset_c = action_to_offset[action]\n",
        "\n",
        "        #get the new row and column after applying the offsets \n",
        "        new_r, new_c = state_r + offset_r, state_c + offset_c\n",
        "\n",
        "        # if the new row and column are in bounds\n",
        "        if self._inbounds_rc(new_r, new_c) and state not in self._blocked_cells:\n",
        "            #get the new state from the new row and new column\n",
        "            new_state = new_r * self._width + new_c\n",
        "            return new_state\n",
        "        else:\n",
        "            return state\n",
        "        \n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        #To Do:\n",
        "        if state == self._goal_cell or state in self._danger_cells:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #create a an array to store the non-terminal states\n",
        "        non_terminal_state = []\n",
        "        #loop through all states in the grid\n",
        "        for state in range(len(self._grid_values)):\n",
        "            #if the state is not terminal then add to non_terminal_state array\n",
        "            if self.is_terminal(state) == False and state not in self._blocked_cells:\n",
        "                non_terminal_state.append(state)\n",
        "        #return non_terminal_state array\n",
        "        return non_terminal_state\n",
        "    \n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #create an array to store valid actions\n",
        "        if self.is_terminal(state):\n",
        "            return []\n",
        "        else:\n",
        "            return list(Action)\n",
        "            \n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        #TO DO:\n",
        "        #if the state is in a goal cell (non-zero reward) return the reward value\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        #if the state is in a danger cell (non-zero reward) return the reward value\n",
        "        elif state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        # else return 0 as there is no reward\n",
        "        else:\n",
        "            return 0 \n",
        "        \n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state\n",
        "        Each item in the list is a dictionary, containing the probability of reaching that state and the state itself\n",
        "        \"\"\"\n",
        "\n",
        "        transitions = {}\n",
        "\n",
        "        new_state = self._state_from_action(state, action)\n",
        "\n",
        "        if self._noise == 0:\n",
        "            return [{new_state: 1.0}]  # Deterministic case\n",
        "\n",
        "        # Compute probability for the intended action\n",
        "        transitions[new_state] = 1 - self._noise\n",
        "\n",
        "        # Compute probability for noisy actions\n",
        "        noise_prob = self._noise / (len(Action) - 1)\n",
        "\n",
        "        # loop through all actions\n",
        "        for noise_action in Action:\n",
        "            # if the noise action is not the same as the action\n",
        "            if noise_action != action:\n",
        "                # get the noisy state\n",
        "                noisy_state = self._state_from_action(state, noise_action)\n",
        "\n",
        "                # If the noisy state is already in the dictionary, accumulate probability\n",
        "                if noisy_state in transitions:\n",
        "                    transitions[noisy_state] += noise_prob\n",
        "                else:\n",
        "                    transitions[noisy_state] = noise_prob\n",
        "\n",
        "        # Return as a list of dictionaries, as expected\n",
        "        return [{state: prob} for state, prob in transitions.items()]\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating\n",
        "        If this is not used, then asynchronous updating may result in unexpected results\n",
        "        To use properly, run this at the start of each iteration\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #create a copy of the grid values\n",
        "        self._next_values = deepcopy(self._grid_values)\n",
        "\n",
        "\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values\n",
        "        To use properly, run this at the end of each iteration\n",
        "        \"\"\"\n",
        "        # TO DO:\n",
        "        #set the grid values to the next values\n",
        "        self._grid_values = self._next_values\n",
        "        \n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy\n",
        "        This value will not update into main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        #TO DO:\n",
        "        #set the value of the state to the next values\n",
        "        self._next_values[state] = value\n",
        "\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        #To Do:\n",
        "        #grab every state not just non-teminal states\n",
        "        # non_terminal_states = self.get_states()\n",
        "        create_next_values = self.create_next_values()\n",
        "        states = range(self._height * self._width)\n",
        "        num_states = len(states)\n",
        "\n",
        "        # create matrix A representing the coeefficients of the linear system\n",
        "        A = np.zeros((num_states, num_states))\n",
        "        # create vector b representing the expected reward\n",
        "        b = np.zeros(num_states)\n",
        "\n",
        "        # loop through the states\n",
        "        for state in states:\n",
        "            reward = self.get_reward(state)\n",
        "            # if the state is terminal \n",
        "            if self.is_terminal(state):\n",
        "                # the value is unchanged\n",
        "                A[state, state] = 1\n",
        "                # value of the terminal state is the expected reward\n",
        "                b[state] = self.get_reward(state)\n",
        "            # if the state is not terminal \n",
        "            else:\n",
        "                # get the actions that the state can take\n",
        "                valid_actions = self.get_actions(state)\n",
        "\n",
        "                # loop through the actions that can be taken from state\n",
        "                for action in valid_actions:\n",
        "                    # get the transitions that can be made from the state and action\n",
        "                    transactions = self.get_transitions(state, action)\n",
        "\n",
        "                    # loop through the transactions\n",
        "                    for transaction in transactions:\n",
        "                        # get the new state and the probability of reaching that state\n",
        "                        new_state, prob = list(transaction.items())[0]\n",
        "                        total_prob = prob * 1/(len(list(Action)))\n",
        "\n",
        "                        # update the A matrix and b vector\n",
        "                        A[state, new_state] -= total_prob * discount_factor\n",
        "                        # expected reward contribution\n",
        "                        b[state] += discount_factor * total_prob * self.get_reward(new_state)\n",
        "            A[state,state] += 1\n",
        "\n",
        "        # solve the linear equation Ax = b\n",
        "        values = np.linalg.solve(A, b)\n",
        "\n",
        "        #update grid world values\n",
        "        for i, state in enumerate(states):\n",
        "            self.set_value(state, values[i])\n",
        "        \n",
        "        self.set_next_values()\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dfgo0v5sNO78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -1.70  -2.87  -5.00  -2.34  -0.00 \n",
            " -0.90   ----   ----  -0.00   2.34 \n",
            " -1.17   ----   ----  -0.00   GOAL \n",
            " -2.87  -3.35  -3.75  -5.00  -0.76 \n",
            " -4.69  -5.00  -4.95  -4.64  -2.44 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "# Solve the linear system\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.95)\n",
        "print(values_grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJI-fizhOIM-"
      },
      "outputs": [],
      "source": [
        "def value_iteration(gw, discount, tolerance=0.1):\n",
        "    #TO DO:\n",
        "\n",
        "    #initialize the grid values\n",
        "    gw.create_next_values()\n",
        "    # keep track of the number of iterations\n",
        "    iterations = 0\n",
        "\n",
        "    while True:\n",
        "        #initialize the delta value\n",
        "        delta = 0\n",
        "        iterations += 1\n",
        "        print(f\"iteration: {iterations}\")\n",
        "        # loop through the states of the grid\n",
        "        for state in gw.get_states():\n",
        "            # keep track of the value at the state\n",
        "            old_value = gw.get_value(state)\n",
        "            # initialize the best value as negative infinity so that we can update it\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            # loop through the actions that can be taken from the state\n",
        "            for action in gw.get_actions(state):\n",
        "                # initialize the expected value\n",
        "                expected_value = 0\n",
        "                # loop through the transactions that can be made from the state \n",
        "                for transaction in gw.get_transitions(state, action):\n",
        "                    # initalize new state and probability of reaching that state\n",
        "                    new_state, prob = list(transaction.items())[0]\n",
        "                    # update the expected value using the equation \n",
        "                    expected_value += prob * (gw.get_reward(new_state) + discount * gw.get_value(new_state))\n",
        "                # update the best value with the maximum value between the best value and the expected value\n",
        "                best_value = max(best_value, expected_value)\n",
        "                # update the value of the state with the best value\n",
        "            gw.set_value(state, best_value)\n",
        "            \n",
        "            # update the delta value\n",
        "            delta = max(delta, abs(best_value - old_value))\n",
        "\n",
        "        gw.set_next_values()\n",
        "        print(f\" Max value Change (delta): {delta}\")\n",
        "\n",
        "        if delta < tolerance:\n",
        "            print(f\"Converged after {iterations} iterations\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OwueBZR8tPXE"
      },
      "outputs": [],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yli-IAo6tPZU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 1\n",
            " Max value Change (delta): 5.0\n",
            "iteration: 2\n",
            " Max value Change (delta): 4.75\n",
            "iteration: 3\n",
            " Max value Change (delta): 4.5125\n",
            "iteration: 4\n",
            " Max value Change (delta): 4.286875\n",
            "iteration: 5\n",
            " Max value Change (delta): 4.07253125\n",
            "iteration: 6\n",
            " Max value Change (delta): 3.8689046874999997\n",
            "iteration: 7\n",
            " Max value Change (delta): 3.6754594531249998\n",
            "iteration: 8\n",
            " Max value Change (delta): 3.4916864804687497\n",
            "iteration: 9\n",
            " Max value Change (delta): 3.317102156445312\n",
            "iteration: 10\n",
            " Max value Change (delta): 3.151247048623046\n",
            "iteration: 11\n",
            " Max value Change (delta): 0\n",
            "Converged after 11 iterations\n",
            "  3.15   2.99  -5.00   4.51   4.75 \n",
            "  3.32   ----   ----   4.75   5.00 \n",
            "  3.49   ----   ----   5.00   GOAL \n",
            "  3.68   3.87   4.07  -5.00   5.00 \n",
            "  3.49  -5.00   4.29   4.51   4.75 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "value_iteration(simple_gw, discount, 0.1)\n",
        "print(simple_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uUo6u5kutPbm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 1\n",
            " Max value Change (delta): 4.0\n",
            "iteration: 2\n",
            " Max value Change (delta): 3.249774074074074\n",
            "iteration: 3\n",
            " Max value Change (delta): 2.659602876543209\n",
            "iteration: 4\n",
            " Max value Change (delta): 1.686456533920798\n",
            "iteration: 5\n",
            " Max value Change (delta): 0.7952664756220722\n",
            "iteration: 6\n",
            " Max value Change (delta): 0.5753639243055478\n",
            "iteration: 7\n",
            " Max value Change (delta): 0.438579171940911\n",
            "iteration: 8\n",
            " Max value Change (delta): 0.312768056228189\n",
            "iteration: 9\n",
            " Max value Change (delta): 0.21252354442549043\n",
            "iteration: 10\n",
            " Max value Change (delta): 0.16891374533558698\n",
            "iteration: 11\n",
            " Max value Change (delta): 0.14617127472623723\n",
            "iteration: 12\n",
            " Max value Change (delta): 0.13107707617565817\n",
            "iteration: 13\n",
            " Max value Change (delta): 0.08555986219867542\n",
            "Converged after 13 iterations\n",
            "  0.31  -0.11  -5.00   3.56   4.48 \n",
            "  0.42   ----   ----   4.16   4.86 \n",
            "  0.51   ----   ----   3.93   GOAL \n",
            "  0.60   0.69   1.30  -5.00   4.17 \n",
            "  0.14  -5.00   2.09   2.90   3.84 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "value_iteration(noisy_gw, discount, 0.1)\n",
        "print(noisy_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4uXK_mV9tPdp"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "incomplete input (634266616.py, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "def policy_iteration(gw, discount, tolerance=0.1):\n",
        "        \n",
        "    \n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
